{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t2t_problem.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/LauraMartinus/ukuxhumana/blob/master/tensor2tensor/notebooks/t2t_problem.ipynb)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Wd48fv-zDMe6"
      },
      "cell_type": "markdown",
      "source": [
        "# Welcome to the [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor) Dataset Colab!\n",
        "\n",
        "Tensor2Tensor, or T2T for short, is a library of deep learning models and datasets designed to make deep learning more accessible and [accelerate ML research](https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html).\n",
        "\n",
        "**This colab shows you how to add your own dataset to T2T so that you can train one of the several preexisting models on your newly added dataset!**\n",
        "\n",
        "For a tutorial that covers all the broader aspects of T2T using existing datasets and models, please see this [IPython notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)."
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "FesA0dakI2kh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Copyright 2018 Google LLC.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "toc",
        "id": "av8U13aqyEdf"
      },
      "cell_type": "markdown",
      "source": [
        ">[Welcome to the Tensor2Tensor Dataset Colab!](#scrollTo=Wd48fv-zDMe6)\n",
        "\n",
        ">>[Installation & Setup](#scrollTo=Urn4QmNfI3hw)\n",
        "\n",
        ">>[Define the Problem](#scrollTo=LUoP57gOjlk9)\n",
        "\n",
        ">>>[Run t2t_datagen](#scrollTo=Q1xBmlrFLSPX)\n",
        "\n",
        ">>[Viewing the generated data.](#scrollTo=MCqJhdnYgiG-)\n",
        "\n",
        ">>>[tf.python_io.tf_record_iterator](#scrollTo=uNpohcPXKsLN)\n",
        "\n",
        ">>>[Using tf.data.Dataset](#scrollTo=6o_1BHGQC5w5)\n",
        "\n",
        ">>[Terminology](#scrollTo=xRtfC0sHBlSo)\n",
        "\n",
        ">>>[Problem](#scrollTo=xRtfC0sHBlSo)\n",
        "\n",
        ">>>[Modalities](#scrollTo=xRtfC0sHBlSo)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Urn4QmNfI3hw"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation & Setup\n",
        "\n",
        "\n",
        "We'll install T2T and TensorFlow.\n",
        "\n",
        "We also need to setup the directories where T2T will:\n",
        "\n",
        "*   Generate the dataset and write the TFRecords file representing the training and the eval set, vocabulary files etc `DATA_DIR`\n",
        "*   Run the training, keep the graph and the checkpoint files `OUTPUT_DIR` and\n",
        "*   Use as a scratch directory to download your dataset from a URL, unzip it, etc. `TMP_DIR`"
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "IBWBeE39JYaR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Run for installation.\n",
        "\n",
        "! pip install -q -U tensor2tensor\n",
        "! pip install -q tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "sbTULiroLs2w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Run this only once - Sets up TF Eager execution.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable Eager execution - useful for seeing the generated data.\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "A8JljOzDYF-Z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Setting a random seed.\n",
        "\n",
        "from tensor2tensor.utils import trainer_lib\n",
        "\n",
        "# Set a seed so that we have deterministic outputs.\n",
        "RANDOM_SEED = 301\n",
        "trainer_lib.set_random_seed(RANDOM_SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "ioW-V1qpqSCE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Run for setting up directories.\n",
        "\n",
        "import os\n",
        "\n",
        "# Setup and create directories.\n",
        "DATA_DIR = os.path.expanduser(\"/tmp/t2t/data\")\n",
        "OUTPUT_DIR = os.path.expanduser(\"/tmp/t2t/output\")\n",
        "TMP_DIR = os.path.expanduser(\"/tmp/t2t/tmp\")\n",
        "\n",
        "# Create them.\n",
        "tf.gfile.MakeDirs(DATA_DIR)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "tf.gfile.MakeDirs(TMP_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LUoP57gOjlk9"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the `Problem`\n",
        "\n",
        "To simplify our setting our input text sampled randomly from [a, z] - each sentence has between [3, 20] words with each word being [1, 8] characters in length.\n",
        "\n",
        "Example input: \"olrkpi z cldv xqcxisg cutzllf doteq\" -- this will be generated by `sample_sentence()`\n",
        "\n",
        "Our output will be the input words sorted according to length.\n",
        "\n",
        "Example output: \"z cldv doteq olrkpi xqcxisg cutzllf\" -- this will be processed by `target_sentence()`\n",
        "\n",
        "Let's dive right into our first problem -- we'll explain as we go on.\n",
        "\n",
        "Take some time to read each line along with its comments -- or skip them and come back later to clarify your understanding."
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "pDDiPxqg9UF-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Define `sample_sentence()` and `target_sentence(input_sentence)`\n",
        "import random\n",
        "import string\n",
        "\n",
        "def sample_sentence():\n",
        "    # Our sentence has between 3 and 20 words\n",
        "    num_words = random.randint(3, 20)\n",
        "    words = []\n",
        "    for i in range(num_words):\n",
        "        # Our words have between 1 and 8 characters.\n",
        "        num_chars = random.randint(1, 8)\n",
        "        chars = []\n",
        "        for j in range(num_chars):\n",
        "            chars.append(random.choice(string.ascii_lowercase))\n",
        "        words.append(\"\".join(chars))\n",
        "    return \" \".join(words)\n",
        "\n",
        "def target_sentence(input_sentence):\n",
        "    words = input_sentence.split(\" \")\n",
        "    return \" \".join(sorted(words, key=lambda x: len(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KcT_x4ma-Uaq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# `Problem` is the base class for any dataset that we want to add to T2T -- it\n",
        "# unifies the specification of the problem for generating training data,\n",
        "# training, evaluation and inference.\n",
        "#\n",
        "# All its methods (except `generate_data`) have reasonable default\n",
        "# implementations.\n",
        "#\n",
        "# A sub-class must implement `generate_data(data_dir, tmp_dir)` -- this method\n",
        "# is called by t2t-trainer or t2t-datagen to actually generate TFRecord dataset\n",
        "# files on disk.\n",
        "from tensor2tensor.data_generators import problem\n",
        "\n",
        "# Certain categories of problems are very common, like where either the input or\n",
        "# output is text, for such problems we define an (abstract) sub-class of\n",
        "# `Problem` called `Text2TextProblem` -- this implements `generate_data` in\n",
        "# terms of another function `generate_samples`. Sub-classes must override\n",
        "# `generate_samples` and `is_generate_per_split`.\n",
        "from tensor2tensor.data_generators import text_problems\n",
        "\n",
        "# Every non-abstract problem sub-class (as well as models and hyperparameter\n",
        "# sets) must be registered with T2T so that T2T knows about it and can look it\n",
        "# up when you specify your problem on the commandline to t2t-trainer or\n",
        "# t2t-datagen.\n",
        "#\n",
        "# One uses:\n",
        "# `register_problem` for a new Problem sub-class.\n",
        "# `register_model` for a new T2TModel sub-class.\n",
        "# `register_hparams` for a new hyperparameter set. All hyperparameter sets\n",
        "# typically extend `common_hparams.basic_params1` (directly or indirectly).\n",
        "from tensor2tensor.utils import registry\n",
        "\n",
        "from tensor2tensor.data_generators import text_encoder\n",
        "\n",
        "from tensor2tensor.data_generators import translate\n",
        "\n",
        "# By default, when you register a problem (or model or hyperparameter set) the\n",
        "# name with which it gets registered is the 'snake case' version -- so here\n",
        "# the Problem class `SortWordsAccordingToLengthRandom` will be registered with\n",
        "# the name `sort_words_according_to_length_random`.\n",
        "#\n",
        "# One can override this default by actually assigning a name as follows:\n",
        "# `@registry.register_problem(\"my_awesome_problem\")`\n",
        "\n",
        "# End-of-sentence marker.\n",
        "EOS = text_encoder.EOS_ID\n",
        "\n",
        "\n",
        "_ENTN_TRAIN_DATASETS = [\n",
        "    [\n",
        "        \"https://github.com/LauraMartinus/ukuxhumana/blob/master/data/eng_tswane/eng_tswane.train.tar.gz?raw=true\",\n",
        "        (\n",
        "            \"entn_parallel.train.en\",\n",
        "            \"entn_parallel.train.tn\"\n",
        "        )\n",
        "    ]\n",
        "]\n",
        "\n",
        "_ENTN_TEST_DATASETS = [\n",
        "    [\n",
        "        \"https://github.com/LauraMartinus/ukuxhumana/blob/master/data/eng_tswane/eng_tswane.dev.tar.gz?raw=true\",\n",
        "        (\n",
        "            \"entn_parallel.dev.en\",\n",
        "            \"entn_parallel.dev.tn\"\n",
        "        )\n",
        "    ]\n",
        "]\n",
        "\n",
        "\n",
        "@registry.register_problem\n",
        "class TranslateEntnRma(translate.TranslateProblem):\n",
        "  \"\"\"Problem spec for WMT English-Tswane translation.\"\"\"\n",
        "\n",
        "  @property\n",
        "  def approx_vocab_size(self):\n",
        "    return 2**15  # 32768\n",
        "\n",
        "  @property\n",
        "  def vocab_filename(self):\n",
        "    return \"vocab.entn.%d\" % self.approx_vocab_size\n",
        "\n",
        "\n",
        "  def source_data_files(self, dataset_split):\n",
        "    train = dataset_split == problem.DatasetSplit.TRAIN\n",
        "    return _ENTN_TRAIN_DATASETS if train else _ENTN_TEST_DATASETS\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HwxQpOKhrolK"
      },
      "cell_type": "markdown",
      "source": [
        "That's it!\n",
        "\n",
        "To use this with `t2t-trainer` or `t2t-datagen`, save it to a directory, add an `__init__.py` that imports it, and then specify that directory with `--t2t_usr_dir`.\n",
        "\n",
        "i.e. as follows:\n",
        "\n",
        "```\n",
        "$ t2t-datagen \\\n",
        "  --problem=sort_words_according_to_length_random \\\n",
        "  --data_dir=/tmp/t2t/data \\\n",
        "  --tmp_dir=/tmp/t2t/tmp \\\n",
        "  --t2t_usr_dir=/tmp/t2t/usr\n",
        "\n",
        "```\n",
        "\n",
        "However, we'll generate the data from the colab itself as well -- this is what `t2t-datagen` essentially does."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Q1xBmlrFLSPX"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate the data.\n",
        "\n",
        "We will now generate the data by calling `Problem.generate_data()` and inspect it."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "atYWRpM1FgaJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1837
        },
        "outputId": "cfa8b91a-f089-4396-8711-4adfbccc3199"
      },
      "cell_type": "code",
      "source": [
        "entn = TranslateEntnRma()\n",
        "\n",
        "entn.generate_data(DATA_DIR, TMP_DIR)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Downloading https://github.com/LauraMartinus/ukuxhumana/blob/master/data/eng_tswane/eng_tswane.train.tar.gz?raw=true to /tmp/t2t/tmp/eng_tswane.train.tar.gz?raw=true\n",
            "100% completed\n",
            "INFO:tensorflow:Successfully downloaded eng_tswane.train.tar.gz?raw=true, 7524431 bytes.\n",
            "INFO:tensorflow:Generating vocab file: /tmp/t2t/data/vocab.entn.32768\n",
            "INFO:tensorflow:Generating vocab from: [['https://github.com/LauraMartinus/ukuxhumana/blob/master/data/eng_tswane/eng_tswane.train.tar.gz?raw=true', ('entn_parallel.train.en', 'entn_parallel.train.tn')]]\n",
            "INFO:tensorflow:Not downloading, file already found: /tmp/t2t/tmp/eng_tswane.train.tar.gz?raw=true\n",
            "INFO:tensorflow:Reading file: entn_parallel.train.en\n",
            "INFO:tensorflow:Reading file: entn_parallel.train.tn\n",
            "INFO:tensorflow:Trying min_count 500\n",
            "INFO:tensorflow:Iteration 0\n",
            "INFO:tensorflow:vocab_size = 1174\n",
            "INFO:tensorflow:Iteration 1\n",
            "INFO:tensorflow:vocab_size = 633\n",
            "INFO:tensorflow:Iteration 2\n",
            "INFO:tensorflow:vocab_size = 685\n",
            "INFO:tensorflow:Iteration 3\n",
            "INFO:tensorflow:vocab_size = 674\n",
            "INFO:tensorflow:Trying min_count 250\n",
            "INFO:tensorflow:Iteration 0\n",
            "INFO:tensorflow:vocab_size = 2208\n",
            "INFO:tensorflow:Iteration 1\n",
            "INFO:tensorflow:vocab_size = 1033\n",
            "INFO:tensorflow:Iteration 2\n",
            "INFO:tensorflow:vocab_size = 1131\n",
            "INFO:tensorflow:Iteration 3\n",
            "INFO:tensorflow:vocab_size = 1103\n",
            "INFO:tensorflow:Trying min_count 125\n",
            "INFO:tensorflow:Iteration 0\n",
            "INFO:tensorflow:vocab_size = 3942\n",
            "INFO:tensorflow:Iteration 1\n",
            "INFO:tensorflow:vocab_size = 1724\n",
            "INFO:tensorflow:Iteration 2\n",
            "INFO:tensorflow:vocab_size = 1843\n",
            "INFO:tensorflow:Iteration 3\n",
            "INFO:tensorflow:vocab_size = 1825\n",
            "INFO:tensorflow:Trying min_count 62\n",
            "INFO:tensorflow:Iteration 0\n",
            "INFO:tensorflow:vocab_size = 6880\n",
            "INFO:tensorflow:Iteration 1\n",
            "INFO:tensorflow:vocab_size = 2821\n",
            "INFO:tensorflow:Iteration 2\n",
            "INFO:tensorflow:vocab_size = 3008\n",
            "INFO:tensorflow:Iteration 3\n",
            "INFO:tensorflow:vocab_size = 2964\n",
            "INFO:tensorflow:Trying min_count 31\n",
            "INFO:tensorflow:Iteration 0\n",
            "INFO:tensorflow:vocab_size = 11435\n",
            "INFO:tensorflow:Iteration 1\n",
            "INFO:tensorflow:vocab_size = 4523\n",
            "INFO:tensorflow:Iteration 2\n",
            "INFO:tensorflow:vocab_size = 4732\n",
            "INFO:tensorflow:Iteration 3\n",
            "INFO:tensorflow:vocab_size = 4700\n",
            "INFO:tensorflow:Trying min_count 15\n",
            "INFO:tensorflow:Iteration 0\n",
            "INFO:tensorflow:vocab_size = 18732\n",
            "INFO:tensorflow:Iteration 1\n",
            "INFO:tensorflow:vocab_size = 7221\n",
            "INFO:tensorflow:Iteration 2\n",
            "INFO:tensorflow:vocab_size = 7496\n",
            "INFO:tensorflow:Iteration 3\n",
            "INFO:tensorflow:vocab_size = 7456\n",
            "INFO:tensorflow:Trying min_count 7\n",
            "INFO:tensorflow:Iteration 0\n",
            "INFO:tensorflow:vocab_size = 29778\n",
            "INFO:tensorflow:Iteration 1\n",
            "INFO:tensorflow:vocab_size = 11438\n",
            "INFO:tensorflow:Iteration 2\n",
            "INFO:tensorflow:vocab_size = 11816\n",
            "INFO:tensorflow:Iteration 3\n",
            "INFO:tensorflow:vocab_size = 11752\n",
            "INFO:tensorflow:Trying min_count 3\n",
            "INFO:tensorflow:Iteration 0\n",
            "INFO:tensorflow:vocab_size = 48004\n",
            "INFO:tensorflow:Iteration 1\n",
            "INFO:tensorflow:vocab_size = 18398\n",
            "INFO:tensorflow:Iteration 2\n",
            "INFO:tensorflow:vocab_size = 18842\n",
            "INFO:tensorflow:Iteration 3\n",
            "INFO:tensorflow:vocab_size = 18758\n",
            "INFO:tensorflow:Trying min_count 1\n",
            "INFO:tensorflow:Iteration 0\n",
            "INFO:tensorflow:vocab_size = 82044\n",
            "INFO:tensorflow:Iteration 1\n",
            "INFO:tensorflow:vocab_size = 27854\n",
            "INFO:tensorflow:Iteration 2\n",
            "INFO:tensorflow:vocab_size = 27854\n",
            "INFO:tensorflow:Iteration 3\n",
            "INFO:tensorflow:vocab_size = 27854\n",
            "INFO:tensorflow:Generating case 0.\n",
            "INFO:tensorflow:Generating case 100000.\n",
            "INFO:tensorflow:Generated 111300 Examples\n",
            "INFO:tensorflow:Downloading https://github.com/LauraMartinus/ukuxhumana/blob/master/data/eng_tswane/eng_tswane.dev.tar.gz?raw=true to /tmp/t2t/tmp/eng_tswane.dev.tar.gz?raw=true\n",
            "100% completed\n",
            "INFO:tensorflow:Successfully downloaded eng_tswane.dev.tar.gz?raw=true, 2477907 bytes.\n",
            "INFO:tensorflow:Found vocab file: /tmp/t2t/data/vocab.entn.32768\n",
            "INFO:tensorflow:Generating case 0.\n",
            "INFO:tensorflow:Generated 47700 Examples\n",
            "INFO:tensorflow:Shuffling data...\n",
            "INFO:tensorflow:Data shuffled.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MCqJhdnYgiG-"
      },
      "cell_type": "markdown",
      "source": [
        "## Viewing the generated data.\n",
        "\n",
        "`tf.data.Dataset` is the recommended API for inputting data into a TensorFlow graph and the `Problem.dataset()` method returns a `tf.data.Dataset` object.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PZczDWnOQDp2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "0326362e-e9ae-4a2a-edcd-ee5707eb7369"
      },
      "cell_type": "code",
      "source": [
        "tfe = tf.contrib.eager\n",
        "\n",
        "Modes = tf.estimator.ModeKeys\n",
        "\n",
        "# We can iterate over our examples by making an iterator and calling next on it.\n",
        "eager_iterator = tfe.Iterator(entn.dataset(Modes.EVAL, DATA_DIR))\n",
        "example = eager_iterator.next()\n",
        "\n",
        "input_tensor = example[\"inputs\"]\n",
        "target_tensor = example[\"targets\"]\n",
        "\n",
        "# The tensors are actually encoded using the generated vocabulary file -- you\n",
        "# can inspect the actual vocab file in DATA_DIR.\n",
        "print(\"Tensor Input: \" + str(input_tensor))\n",
        "print(\"Tensor Target: \" + str(target_tensor))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /tmp/t2t/data/translate_entn_rma-dev*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 1\n",
            "Tensor Input: tf.Tensor([9993    3   84   26 4506   13    4 5214 3118    2    1], shape=(11,), dtype=int64)\n",
            "Tensor Target: tf.Tensor([  167   156     3     8 20275     7  2308    27  1339     2     1], shape=(11,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8PUshK-jZwr7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5669
        },
        "outputId": "136113b4-3269-4c73-c8d5-887a6e2e974a"
      },
      "cell_type": "code",
      "source": [
        "from tensor2tensor import problems\n",
        "problems.available()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/video_generated.py:33: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
            "    \"__main__\", fname, loader, pkg_name)\n",
            "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
            "    exec code in run_globals\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 657, in launch_instance\n",
            "    app.initialize(argv)\n",
            "  File \"<decorator-gen-121>\", line 2, in initialize\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 87, in catch_config_error\n",
            "    return method(app, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 462, in initialize\n",
            "    self.init_gui_pylab()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 403, in init_gui_pylab\n",
            "    InteractiveShellApp.init_gui_pylab(self)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/shellapp.py\", line 213, in init_gui_pylab\n",
            "    r = enable(key)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
            "    pt.activate_matplotlib(backend)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
            "    matplotlib.pyplot.switch_backend(backend)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
            "    matplotlib.use(newbackend, warn=False, force=True)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
            "    reload(sys.modules['matplotlib.backends'])\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  matplotlib.use(\"agg\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['algorithmic_addition_binary40',\n",
              " 'algorithmic_addition_decimal40',\n",
              " 'algorithmic_cipher_shift200',\n",
              " 'algorithmic_cipher_shift5',\n",
              " 'algorithmic_cipher_vigenere200',\n",
              " 'algorithmic_cipher_vigenere5',\n",
              " 'algorithmic_identity_binary40',\n",
              " 'algorithmic_identity_decimal40',\n",
              " 'algorithmic_multiplication_binary40',\n",
              " 'algorithmic_multiplication_decimal40',\n",
              " 'algorithmic_reverse_binary40',\n",
              " 'algorithmic_reverse_binary40_test',\n",
              " 'algorithmic_reverse_decimal40',\n",
              " 'algorithmic_reverse_nlplike32k',\n",
              " 'algorithmic_reverse_nlplike8k',\n",
              " 'algorithmic_shift_decimal40',\n",
              " 'algorithmic_sort_problem',\n",
              " 'audio_timit_characters_tune',\n",
              " 'audio_timit_tokens8k_test',\n",
              " 'audio_timit_tokens8k_tune',\n",
              " 'babi_qa_concat_all_tasks_10k',\n",
              " 'babi_qa_concat_all_tasks_1k',\n",
              " 'babi_qa_concat_task10_10k',\n",
              " 'babi_qa_concat_task10_1k',\n",
              " 'babi_qa_concat_task11_10k',\n",
              " 'babi_qa_concat_task11_1k',\n",
              " 'babi_qa_concat_task12_10k',\n",
              " 'babi_qa_concat_task12_1k',\n",
              " 'babi_qa_concat_task13_10k',\n",
              " 'babi_qa_concat_task13_1k',\n",
              " 'babi_qa_concat_task14_10k',\n",
              " 'babi_qa_concat_task14_1k',\n",
              " 'babi_qa_concat_task15_10k',\n",
              " 'babi_qa_concat_task15_1k',\n",
              " 'babi_qa_concat_task16_10k',\n",
              " 'babi_qa_concat_task16_1k',\n",
              " 'babi_qa_concat_task17_10k',\n",
              " 'babi_qa_concat_task17_1k',\n",
              " 'babi_qa_concat_task18_10k',\n",
              " 'babi_qa_concat_task18_1k',\n",
              " 'babi_qa_concat_task19_10k',\n",
              " 'babi_qa_concat_task19_1k',\n",
              " 'babi_qa_concat_task1_10k',\n",
              " 'babi_qa_concat_task1_1k',\n",
              " 'babi_qa_concat_task20_10k',\n",
              " 'babi_qa_concat_task20_1k',\n",
              " 'babi_qa_concat_task2_10k',\n",
              " 'babi_qa_concat_task2_1k',\n",
              " 'babi_qa_concat_task3_10k',\n",
              " 'babi_qa_concat_task3_1k',\n",
              " 'babi_qa_concat_task4_10k',\n",
              " 'babi_qa_concat_task4_1k',\n",
              " 'babi_qa_concat_task5_10k',\n",
              " 'babi_qa_concat_task5_1k',\n",
              " 'babi_qa_concat_task6_10k',\n",
              " 'babi_qa_concat_task6_1k',\n",
              " 'babi_qa_concat_task7_10k',\n",
              " 'babi_qa_concat_task7_1k',\n",
              " 'babi_qa_concat_task8_10k',\n",
              " 'babi_qa_concat_task8_1k',\n",
              " 'babi_qa_concat_task9_10k',\n",
              " 'babi_qa_concat_task9_1k',\n",
              " 'cola',\n",
              " 'cola_characters',\n",
              " 'common_voice',\n",
              " 'common_voice_clean',\n",
              " 'common_voice_noisy',\n",
              " 'common_voice_train_full_test_clean',\n",
              " 'genomics_expression_cage10',\n",
              " 'genomics_expression_gm12878',\n",
              " 'genomics_expression_l262k',\n",
              " 'github_function_docstring',\n",
              " 'gym_boxing-v0_random',\n",
              " 'gym_boxing-v4_random',\n",
              " 'gym_boxing_deterministic-v0_random',\n",
              " 'gym_boxing_deterministic-v4_random',\n",
              " 'gym_boxing_no_frameskip-v0_random',\n",
              " 'gym_boxing_no_frameskip-v4_random',\n",
              " 'gym_discrete_problem_with_agent_on_boxing-v0',\n",
              " 'gym_discrete_problem_with_agent_on_boxing-v4',\n",
              " 'gym_discrete_problem_with_agent_on_boxing_deterministic-v0',\n",
              " 'gym_discrete_problem_with_agent_on_boxing_deterministic-v4',\n",
              " 'gym_discrete_problem_with_agent_on_boxing_no_frameskip-v0',\n",
              " 'gym_discrete_problem_with_agent_on_boxing_no_frameskip-v4',\n",
              " 'gym_discrete_problem_with_agent_on_pong-v0',\n",
              " 'gym_discrete_problem_with_agent_on_pong-v4',\n",
              " 'gym_discrete_problem_with_agent_on_pong_deterministic-v0',\n",
              " 'gym_discrete_problem_with_agent_on_pong_deterministic-v4',\n",
              " 'gym_discrete_problem_with_agent_on_pong_no_frameskip-v0',\n",
              " 'gym_discrete_problem_with_agent_on_pong_no_frameskip-v4',\n",
              " 'gym_discrete_problem_with_agent_on_wrapped_full_pong',\n",
              " 'gym_discrete_problem_with_agent_on_wrapped_full_pong_autoencoded',\n",
              " 'gym_discrete_problem_with_agent_on_wrapped_full_pong_with_autoencoder',\n",
              " 'gym_pong-v0_random',\n",
              " 'gym_pong-v4_random',\n",
              " 'gym_pong_deterministic-v0_random',\n",
              " 'gym_pong_deterministic-v4_random',\n",
              " 'gym_pong_no_frameskip-v0_random',\n",
              " 'gym_pong_no_frameskip-v4_random',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_boxing-v0',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_boxing-v4',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_boxing_deterministic-v0',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_boxing_deterministic-v4',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_boxing_no_frameskip-v0',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_boxing_no_frameskip-v4',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_pong-v0',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_pong-v4',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_pong_deterministic-v0',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_pong_deterministic-v4',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_pong_no_frameskip-v0',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_pong_no_frameskip-v4',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_wrapped_full_pong',\n",
              " 'gym_simulated_discrete_problem_with_agent_on_wrapped_full_pong_autoencoded',\n",
              " 'gym_wrapped_full_pong_random',\n",
              " 'image_celeba',\n",
              " 'image_celeba32',\n",
              " 'image_celeba64',\n",
              " 'image_celeba_multi_resolution',\n",
              " 'image_celebahq128',\n",
              " 'image_celebahq128_dmol',\n",
              " 'image_celebahq256',\n",
              " 'image_celebahq256_dmol',\n",
              " 'image_cifar10',\n",
              " 'image_cifar100',\n",
              " 'image_cifar100_plain',\n",
              " 'image_cifar100_plain8',\n",
              " 'image_cifar100_plain_gen',\n",
              " 'image_cifar100_tune',\n",
              " 'image_cifar10_plain',\n",
              " 'image_cifar10_plain8',\n",
              " 'image_cifar10_plain_gen',\n",
              " 'image_cifar10_plain_gen_dmol',\n",
              " 'image_cifar10_plain_random_shift',\n",
              " 'image_cifar10_tune',\n",
              " 'image_cifar20',\n",
              " 'image_cifar20_plain',\n",
              " 'image_cifar20_plain8',\n",
              " 'image_cifar20_plain_gen',\n",
              " 'image_cifar20_tune',\n",
              " 'image_fashion_mnist',\n",
              " 'image_fsns',\n",
              " 'image_imagenet',\n",
              " 'image_imagenet224',\n",
              " 'image_imagenet32',\n",
              " 'image_imagenet32_gen',\n",
              " 'image_imagenet32_small',\n",
              " 'image_imagenet64',\n",
              " 'image_imagenet64_gen',\n",
              " 'image_imagenet_multi_resolution_gen',\n",
              " 'image_lsun_bedrooms',\n",
              " 'image_mnist',\n",
              " 'image_mnist_tune',\n",
              " 'image_ms_coco_characters',\n",
              " 'image_ms_coco_tokens32k',\n",
              " 'image_text_ms_coco',\n",
              " 'image_text_ms_coco_multi_resolution',\n",
              " 'image_vqav2_rcnn_feature_tokens10k_labels3k',\n",
              " 'image_vqav2_tokens10k_labels3k',\n",
              " 'img2img_allen_brain',\n",
              " 'img2img_allen_brain_dim16to16_paint1',\n",
              " 'img2img_allen_brain_dim48to64',\n",
              " 'img2img_allen_brain_dim8to32',\n",
              " 'img2img_celeba',\n",
              " 'img2img_celeba64',\n",
              " 'img2img_cifar10',\n",
              " 'img2img_cifar100',\n",
              " 'img2img_imagenet',\n",
              " 'lambada_lm',\n",
              " 'lambada_lm_control',\n",
              " 'lambada_rc',\n",
              " 'lambada_rc_control',\n",
              " 'languagemodel_lm1b32k',\n",
              " 'languagemodel_lm1b32k_packed',\n",
              " 'languagemodel_lm1b8k',\n",
              " 'languagemodel_lm1b8k_packed',\n",
              " 'languagemodel_lm1b_characters',\n",
              " 'languagemodel_lm1b_characters_packed',\n",
              " 'languagemodel_lm1b_multi_nli',\n",
              " 'languagemodel_lm1b_multi_nli_subwords',\n",
              " 'languagemodel_lm1b_sentiment_imdb',\n",
              " 'languagemodel_ptb10k',\n",
              " 'languagemodel_ptb_characters',\n",
              " 'languagemodel_wiki_noref_v128k_l1k',\n",
              " 'languagemodel_wiki_noref_v32k_l1k',\n",
              " 'languagemodel_wiki_noref_v8k_l16k',\n",
              " 'languagemodel_wiki_noref_v8k_l1k',\n",
              " 'languagemodel_wiki_scramble_l128',\n",
              " 'languagemodel_wiki_scramble_l1k',\n",
              " 'languagemodel_wiki_xml_v8k_l1k',\n",
              " 'languagemodel_wiki_xml_v8k_l4k',\n",
              " 'languagemodel_wikitext103',\n",
              " 'languagemodel_wikitext103_characters',\n",
              " 'librispeech',\n",
              " 'librispeech_clean',\n",
              " 'librispeech_clean_small',\n",
              " 'librispeech_noisy',\n",
              " 'librispeech_train_full_test_clean',\n",
              " 'msr_paraphrase_corpus',\n",
              " 'msr_paraphrase_corpus_characters',\n",
              " 'multi_nli',\n",
              " 'multi_nli_characters',\n",
              " 'multi_nli_shared_vocab',\n",
              " 'ocr_test',\n",
              " 'paraphrase_generation_ms_coco_problem1d',\n",
              " 'paraphrase_generation_ms_coco_problem1d_characters',\n",
              " 'paraphrase_generation_ms_coco_problem2d',\n",
              " 'paraphrase_generation_ms_coco_problem2d_characters',\n",
              " 'parsing_english_ptb16k',\n",
              " 'parsing_english_ptb8k',\n",
              " 'parsing_icelandic16k',\n",
              " 'program_search_algolisp',\n",
              " 'programming_desc2code_cpp',\n",
              " 'programming_desc2code_py',\n",
              " 'question_nli',\n",
              " 'question_nli_characters',\n",
              " 'quora_question_pairs',\n",
              " 'quora_question_pairs_characters',\n",
              " 'rte',\n",
              " 'rte_characters',\n",
              " 'sentiment_imdb',\n",
              " 'sentiment_imdb_characters',\n",
              " 'sentiment_sst_binary',\n",
              " 'sentiment_sst_binary_characters',\n",
              " 'squad',\n",
              " 'squad_concat',\n",
              " 'squad_concat_positioned',\n",
              " 'stanford_nli',\n",
              " 'stanford_nli_characters',\n",
              " 'stanford_nli_shared_vocab',\n",
              " 'style_transfer_modern_to_shakespeare',\n",
              " 'style_transfer_modern_to_shakespeare_characters',\n",
              " 'style_transfer_shakespeare_to_modern',\n",
              " 'style_transfer_shakespeare_to_modern_characters',\n",
              " 'summarize_cnn_dailymail32k',\n",
              " 'sva_language_modeling',\n",
              " 'sva_number_prediction',\n",
              " 'text2text_copyable_tokens',\n",
              " 'text2text_tmpdir',\n",
              " 'text2text_tmpdir_tokens',\n",
              " 'timeseries_synthetic_data_series10_samples100k',\n",
              " 'timeseries_toy_problem',\n",
              " 'tiny_algo',\n",
              " 'translate_encs_wmt32k',\n",
              " 'translate_encs_wmt_characters',\n",
              " 'translate_ende_wmt32k',\n",
              " 'translate_ende_wmt32k_packed',\n",
              " 'translate_ende_wmt8k',\n",
              " 'translate_ende_wmt8k_packed',\n",
              " 'translate_ende_wmt_bpe32k',\n",
              " 'translate_ende_wmt_characters',\n",
              " 'translate_enet_wmt32k',\n",
              " 'translate_enet_wmt_characters',\n",
              " 'translate_enfr_wmt32k',\n",
              " 'translate_enfr_wmt32k_packed',\n",
              " 'translate_enfr_wmt8k',\n",
              " 'translate_enfr_wmt_characters',\n",
              " 'translate_enfr_wmt_small32k',\n",
              " 'translate_enfr_wmt_small8k',\n",
              " 'translate_enfr_wmt_small_characters',\n",
              " 'translate_enid_iwslt32k',\n",
              " 'translate_enmk_setimes32k',\n",
              " 'translate_enmk_setimes_characters',\n",
              " 'translate_entn_rma',\n",
              " 'translate_envi_iwslt32k',\n",
              " 'translate_enzh_wmt32k',\n",
              " 'translate_enzh_wmt8k',\n",
              " 'video_bair_robot_pushing',\n",
              " 'video_bair_robot_pushing_with_actions',\n",
              " 'video_google_robot_pushing',\n",
              " 'video_stochastic_shapes10k',\n",
              " 'video_twentybn',\n",
              " 'wikisum_commoncrawl',\n",
              " 'wikisum_commoncrawl_lead_section',\n",
              " 'wikisum_web',\n",
              " 'wikisum_web_lead_section',\n",
              " 'winograd_nli',\n",
              " 'winograd_nli_characters',\n",
              " 'wsj_parsing']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "t9GuOqVNZ_11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9503
        },
        "outputId": "b15918fb-0aca-44e2-eeac-e89d93e98be4"
      },
      "cell_type": "code",
      "source": [
        "from tensor2tensor.utils import registry\n",
        "registry.list_hparams()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imagetransformer_b10l_dr03_moe_tpu',\n",
              " 'lstm_seq2seq',\n",
              " 'img2img_transformer2d_n24',\n",
              " 'imagetransformer2d_base_8l_8_16_big',\n",
              " 'imagetransformer_sep_channels_8l_8h',\n",
              " 'img2img_transformer2d_n31',\n",
              " 'afx_small_bfloat16',\n",
              " 'imagetransformer_tiny_tpu',\n",
              " 'slicenet_1',\n",
              " 'transformer_big',\n",
              " 'universal_transformer_dwa_base',\n",
              " 'img2img_transformer2d_tiny',\n",
              " 'adaptive_universal_transformer_with_sru_base',\n",
              " 'slicenet_1tiny',\n",
              " 'transformer_teeny',\n",
              " 'universal_transformer_big',\n",
              " 'universal_transformer_mix_after_ut_base',\n",
              " 'imagetransformer_b10l_4h_big_uncond_dr01_tpu',\n",
              " 'vqa_recurrent_self_attention_mix_before_ut',\n",
              " 'transformer_tpu_with_conv',\n",
              " 'next_frame_emily',\n",
              " 'vqa_attention_feature_batch1024_numglimps1',\n",
              " 'autoencoder_ordered_discrete',\n",
              " 'vqa_attention_feature_hidden1024',\n",
              " 'imagetransformer_base_8l_8h_big_cond_dr03_dan',\n",
              " 'mtf_image_transformer_tiny_8gpu',\n",
              " 'adaptive_universal_transformer_concat_tiny',\n",
              " 'adaptive_universal_transformer_base',\n",
              " 'imagetransformer1d_base_8l_64by64',\n",
              " 'imagetransformer_base_14l_8h_big_dr01',\n",
              " 'afx_small',\n",
              " 'vqa_attention_feature_nonormalization',\n",
              " 'aligned_8k',\n",
              " 'imagetransformer_base_imagenet_tpu',\n",
              " 'universal_transformer_mix_before_ut_base',\n",
              " 'mtf_transformer_lm_moe',\n",
              " 'super_lm_base',\n",
              " 'imagetransformer2d_base_8l_8_32_big',\n",
              " 'afx_mimic_adam',\n",
              " 'vqa_attention_feature_batch1024_drop01_dna',\n",
              " 'mtf_transformer_tiny',\n",
              " 'xmoe_top_2',\n",
              " 'attention_lm_moe_base_hybrid',\n",
              " 'imagetransformerpp_base_12l_8h_big_uncond_dr03_dan_m_rel',\n",
              " 'transformer_h16',\n",
              " 'xception_base',\n",
              " 'super_lm_big_tpu',\n",
              " 'distill_resnet_32_to_15_cifar20x5',\n",
              " 'transformer_relative_tiny',\n",
              " 'next_frame_ae',\n",
              " 'transformer_revnet_base',\n",
              " 'vqa_self_attention_feature_batch1024_exp',\n",
              " 'imagetransformer2d_base',\n",
              " 'imagetransformer_b12l_4h_b128_h512_uncond_dr03_tpu',\n",
              " 'imagetransformer_b12l_4h_b128_uncond_dr03_tpu',\n",
              " 'autoencoder_ordered_discrete_hs256',\n",
              " 'transformer_k128',\n",
              " 'transformer_big_enfr',\n",
              " 'shakeshake_big',\n",
              " 'imagetransformerpp_base_10l_8h_big_uncond_dr03_dan_g',\n",
              " 'imagetransformerpp_base_10l_8h_big_uncond_dr03_dan_a',\n",
              " 'imagetransformerpp_base_10l_8h_big_uncond_dr03_dan_b',\n",
              " 'vqa_recurrent_self_attention_drop3',\n",
              " 'discrete_random_action_base',\n",
              " 'transformer_ae_small',\n",
              " 'imagetransformer_base_10l_16h_big_uncond_dr01_imgnet',\n",
              " 'vqa_attention_feature_initializer',\n",
              " 'vqa_self_attention_feature_batch1024_big',\n",
              " 'vqa_attention_feature_lstmlayernorm',\n",
              " 'autoencoder_ordered_text_small',\n",
              " 'autoencoder_stacked',\n",
              " 'vqa_recurrent_self_attention_highway',\n",
              " 'mtf_image_transformer_single',\n",
              " 'imagetransformer_b12l_4h_b128_h512_uncond_dr01_im',\n",
              " 'transformer_timeseries_tpu',\n",
              " 'adaptive_universal_transformer_position_random_timing_tiny',\n",
              " 'attention_lm_moe_24b_diet',\n",
              " 'mtf_transformer_paper_tr_0_mesh_8_v2',\n",
              " 'super_lm_tpu_memtest',\n",
              " 'autoencoder_basic',\n",
              " 'transformer_k256',\n",
              " 'transformer_tpu_bf16_activation',\n",
              " 'universal_transformer_step_sinusoid_timing_tiny',\n",
              " 'attention_lm_translation_full_attention',\n",
              " 'vqa_recurrent_self_attention_l8',\n",
              " 'transformer_common_voice',\n",
              " 'vqa_attention_feature_batch1024_lstmlayernorm',\n",
              " 'transformer_librispeech_v2',\n",
              " 'transformer_librispeech_v1',\n",
              " 'autoencoder_autoregressive',\n",
              " 'afx_factored',\n",
              " 'imagetransformer_base_10l_16h_big_dr01_imgnet',\n",
              " 'transformer_packed_tpu',\n",
              " 'adaptive_universal_transformer_base_dropout03',\n",
              " 'adaptive_universal_transformer_base_dropout05',\n",
              " 'xmoe_wiki_base',\n",
              " 'aligned_pseudolocal_256',\n",
              " 'universal_transformer_position_random_timing_tiny',\n",
              " 'lmx_moe_h1k_f4k_x32',\n",
              " 'vqa_recurrent_self_attention_drop1',\n",
              " 'transformer_l8',\n",
              " 'imagetransformer_sep_channels_8l_tpu',\n",
              " 'transformer_ae_base_noatt',\n",
              " 'multimodel_tiny',\n",
              " 'transformer_l4',\n",
              " 'transformer_l2',\n",
              " 'transformer_nat_big',\n",
              " 'universal_transformer_skip_base',\n",
              " 'revnet_164_cifar',\n",
              " 'transformer_tiny',\n",
              " 'aligned_moe',\n",
              " 'afx_small_p16',\n",
              " 'afx_small_p10',\n",
              " 'afx_small_p11',\n",
              " 'afx_small_p12',\n",
              " 'next_frame_basic_deterministic',\n",
              " 'mtf_transformer_lm_baseline',\n",
              " 'transformer_moe_8k',\n",
              " 'vqa_attention_feature_batch1024',\n",
              " 'xmoe_dense_8k',\n",
              " 'resnet_imagenet_34_td_weight_05_05',\n",
              " 'lmx_base',\n",
              " 'aligned_base',\n",
              " 'img2img_transformer2d_base',\n",
              " 'universal_transformer_teeny',\n",
              " 'transformer_librispeech_tpu_v2',\n",
              " 'transformer_librispeech_tpu_v1',\n",
              " 'xmoe_wiki_x64',\n",
              " 'attention_lm_base',\n",
              " 'vqa_attention_feature_dnz_noscaledp',\n",
              " 'afx_pow08_clip',\n",
              " 'neural_gpu',\n",
              " 'vqa_attention_feature_batch1024_dnz_l2',\n",
              " 'autoencoder_residual_discrete_big',\n",
              " 'resnet_cifar_32_td_weight_05_05',\n",
              " 'universal_transformer_small',\n",
              " 'transformer_hs256',\n",
              " 'mtf_image_transformer_base_imagenet_mp',\n",
              " 'mtf_transformer_paper_tr_0_mesh_8',\n",
              " 'imagetransformerpp_base_8l_8h_big_cond_dr03_dan_a',\n",
              " 'imagetransformer2d_base_8l_8_16',\n",
              " 'attention_lm_moe_base_local',\n",
              " 'attention_lm_moe_large_diet',\n",
              " 'resnet_cifar_32',\n",
              " 'attention_lm_moe_base_ae',\n",
              " 'attention_lm_ae_extended',\n",
              " 'vqa_attention_feature_batch1024_drop01',\n",
              " 'imagetransformerpp_base_10l_8h_big_uncond_dr03_dan',\n",
              " 'img2img_transformer_dilated',\n",
              " 'transformer_ff1024',\n",
              " 'imagetransformer_moe_tiny',\n",
              " 'super_lm_moe_h4',\n",
              " 'lmx_moe',\n",
              " 'imagetransformerpp_base_12l_8h_big_uncond_dr03_dan_k',\n",
              " 'imagetransformerpp_base_12l_8h_big_uncond_dr03_dan_m',\n",
              " 'imagetransformerpp_base_12l_8h_big_uncond_dr03_dan_l',\n",
              " 'mtf_image_transformer_base_imagenet',\n",
              " 'attention_lm_moe_base_memeff',\n",
              " 'universal_transformer_lstm_tall',\n",
              " 'resnet_imagenet_34_td_unit_05_05',\n",
              " 'vqa_self_attention_feature_batch1024',\n",
              " 'img2img_transformer_b3_bs10',\n",
              " 'autoencoder_residual',\n",
              " 'shakeshake_small',\n",
              " 'transformer_h1',\n",
              " 'transformer_h4',\n",
              " 'vqa_recurrent_self_attention_ls2',\n",
              " 'imagetransformer_b12l_4h_b256_uncond_dr03_rel_tpu',\n",
              " 'lstm_attention',\n",
              " 'vqa_attention_numglimps1',\n",
              " 'img2img_transformer_base_tpu',\n",
              " 'afx_adam',\n",
              " 'mtf_image_transformer_base_cifar',\n",
              " 'resnet_18',\n",
              " 'aligned_pos_emb',\n",
              " 'universal_transformer_sepconv_base',\n",
              " 'vqa_self_attention_feature_batch1024_hidden6_big',\n",
              " 'universal_transformer_highway_base',\n",
              " 'transformer_l10',\n",
              " 'transformer_parsing_big',\n",
              " 'shake_shake_quick',\n",
              " 'afx_clip_factored',\n",
              " 'transformer_small_tpu',\n",
              " 'mtf_transformer_paper_tr_m1',\n",
              " 'aligned_local_1k',\n",
              " 'vqa_attention_feature_dnz_l2',\n",
              " 'aligned_memory_efficient',\n",
              " 'mtf_image_transformer_tiny_moe',\n",
              " 'img2img_transformer_b3',\n",
              " 'imagetransformer2d_base_8l_8_16_big_16k',\n",
              " 'resnet_imagenet_34_td_unit_no_drop',\n",
              " 'autoencoder_ordered_text',\n",
              " 'imagetransformer2d_tiny',\n",
              " 'transformer_sketch',\n",
              " 'transformer_symshard_base',\n",
              " 'lmx_h1k_f4k',\n",
              " 'vqa_self_attention_base',\n",
              " 'imagetransformer_imagenet32_base',\n",
              " 'afx_base',\n",
              " 'aligned_no_att',\n",
              " 'next_frame_small',\n",
              " 'imagetransformer_cifar10_base_dmol',\n",
              " 'transformer_base_vq1_16_nb1_packed_nda_b01_scales_dialog',\n",
              " 'img2img_transformer_b3_bs1',\n",
              " 'img2img_transformer_b3_bs2',\n",
              " 'img2img_transformer_b3_bs3',\n",
              " 'img2img_transformer_b3_bs4',\n",
              " 'img2img_transformer_b3_bs5',\n",
              " 'img2img_transformer_b3_bs6',\n",
              " 'img2img_transformer_b3_bs7',\n",
              " 'img2img_transformer_b3_bs8',\n",
              " 'img2img_transformer_b3_bs9',\n",
              " 'transformer_nat_base',\n",
              " 'transformer_nat_small',\n",
              " 'basic_fc_small',\n",
              " 'autoencoder_discrete_pong',\n",
              " 'transformer_small',\n",
              " 'universal_transformer_base',\n",
              " 'vqa_recurrent_self_attention_base',\n",
              " 'transformer_clean',\n",
              " 'xmoe_2d_88',\n",
              " 'transformer_common_voice_tpu',\n",
              " 'next_frame_sampling',\n",
              " 'vqa_attention_feature_batch1024_dnz_noscaledp',\n",
              " 'lmx_moe_h1k_f8k_x16',\n",
              " 'transformer_ff4096',\n",
              " 'super_lm_high_mix',\n",
              " 'transformer_ae_base_ablation_1',\n",
              " 'autoencoder_residual_discrete',\n",
              " 'transformer_ae_base_ablation_3',\n",
              " 'transformer_ae_base_ablation_4',\n",
              " 'transformer_ae_base_ablation_5',\n",
              " 'resnet_imagenet_102',\n",
              " 'lmx_relative',\n",
              " 'lstm_bahdanau_attention',\n",
              " 'transformer_base_single_gpu',\n",
              " 'imagetransformer_base_10l_8h_big_uncond_dr03_dan',\n",
              " 'imagetransformer_base_10l_8h_big_cond_dr03_dan',\n",
              " 'autoencoder_basic_discrete',\n",
              " 'transformer_symshard_sh4',\n",
              " 'autoencoder_ordered_discrete_single',\n",
              " 'transformer_base_vq1_16_nb1_packed_dan_b01_scales',\n",
              " 'attention_lm_hybrid_v2',\n",
              " 'resnet_50',\n",
              " 'multimodel_base',\n",
              " 'transformer_symshard_lm_0',\n",
              " 'attention_lm_moe_base',\n",
              " 'afx_small_p8',\n",
              " 'transformer_big_enfr_tpu',\n",
              " 'imagetransformer_b12l_4h_big_uncond_dr03_tpu',\n",
              " 'next_frame_l2',\n",
              " 'xmoe_top_2_c15',\n",
              " 'lmx_h3k_f12k',\n",
              " 'ppo_base_v1',\n",
              " 'vqa_attention_feature_imagefeat1024',\n",
              " 'transformer_relative',\n",
              " 'imagetransformer_base_8l_8h_big_cond_dr03_dan_dilated_d',\n",
              " 'vqa_attention_feature_dna',\n",
              " 'imagetransformer_base_8l_8h_big_cond_dr03_dan_dilated_b',\n",
              " 'imagetransformer_base_8l_8h_big_cond_dr03_dan_dilated_c',\n",
              " 'mtf_transformer_tiny_8gpu',\n",
              " 'vqa_self_attention_feature',\n",
              " 'vqa_recurrent_self_attention_big',\n",
              " 'imagetransformer_base_rel',\n",
              " 'imagetransformer_base_12l_8h_big',\n",
              " 'transformer_big_tpu',\n",
              " 'imagetransformer2d_base_12l_8_16_big',\n",
              " 'adaptive_universal_transformer_mix_after_ut_base',\n",
              " 'gene_expression_conv_base',\n",
              " 'transformer_ada_lmpackedbase',\n",
              " 'transformer_prepend_v2',\n",
              " 'transformer_prepend_v1',\n",
              " 'ppo_continuous_action_base',\n",
              " 'vqa_recurrent_self_attention_gru',\n",
              " 'imagetransformerpp_sep_channels_8l_8h',\n",
              " 'lmx_relative_nopos',\n",
              " 'transformer_ae_base',\n",
              " 'attention_lm_moe_tiny',\n",
              " 'imagetransformer_cifar10_base',\n",
              " 'vqa_recurrent_self_attention_big_l4',\n",
              " 'transformer_tiny_tpu',\n",
              " 'ppo_atari_base',\n",
              " 'transformer_moe_2k',\n",
              " 'autoencoder_residual_text',\n",
              " 'super_lm_low_mix',\n",
              " 'transformer_ae_base_iaf',\n",
              " 'xmoe_wiki_x256',\n",
              " 'imagetransformerpp_base_5l_8h_big_uncond_dr00_dan_g_bs1',\n",
              " 'transformer_hs1024',\n",
              " 'transformer_ae_base_ablation_2',\n",
              " 'mtf_transformer_paper_tr_0_mesh_512',\n",
              " 'afx_unscale_relative',\n",
              " 'imagetransformer_base',\n",
              " 'transformer_supervised_attention',\n",
              " 'transformer_ae_small_noatt',\n",
              " 'lstm_asr_v1',\n",
              " 'glow_hparams',\n",
              " 'transformer_base_v2',\n",
              " 'transformer_base_v1',\n",
              " 'transformer_clean_big',\n",
              " 'imagetransformer_sep_channels_8l_multipos3',\n",
              " 'transformer_moe_base',\n",
              " 'imagetransformerpp_base_14l_8h_big_uncond_dr03_dan_p',\n",
              " 'mtf_image_transformer_base_single',\n",
              " 'resnet_152',\n",
              " 'next_frame_l1',\n",
              " 'vqa_recurrent_self_attention_l4',\n",
              " 'universal_transformer_tall',\n",
              " 'transformer_moe_8k_lm',\n",
              " 'vqa_attention_feature_batch512',\n",
              " 'img2img_transformer2d_n103',\n",
              " 'revnet_104',\n",
              " 'aligned_grouped',\n",
              " 'sliced_gan',\n",
              " 'attention_lm_11k',\n",
              " 'lstm_luong_attention_multi',\n",
              " 'mtf_transformer_paper_lm_1',\n",
              " 'mtf_transformer_paper_lm_0',\n",
              " 'mtf_transformer_paper_lm_3',\n",
              " 'mtf_transformer_paper_lm_2',\n",
              " 'mtf_transformer_paper_lm_5',\n",
              " 'mtf_transformer_paper_lm_4',\n",
              " 'lstm_bahdanau_attention_multi',\n",
              " 'transformer_timeseries',\n",
              " 'next_frame_tiny',\n",
              " 'transformer_librispeech',\n",
              " 'adaptive_universal_transformer_global_base',\n",
              " 'transformer_relative_big',\n",
              " 'imagetransformer_b12l_4h_big_uncond_dr03_lr025_tpu',\n",
              " 'image_transformer_base',\n",
              " 'transformer_ae_base_tpu',\n",
              " 'imagetransformer_base_10l_8h_big_uncond_dr03_dan_64',\n",
              " 'imagetransformer_ae_cifar',\n",
              " 'imagetransformer_sep_output_channels_8l_local_and_global_att',\n",
              " 'transformer_h32',\n",
              " 'transformer_ada_lmpackedbase_relative',\n",
              " 'adaptive_universal_transformer_tall',\n",
              " 'imagetransformer_b10l_4h_big_uncond_dr03_lr025_tpu',\n",
              " 'attention_lm_moe_unscramble_base',\n",
              " 'mtf_transformer_paper_tr_6_mesh_64_8',\n",
              " 'next_frame_savp',\n",
              " 'xmoe_wiki_f64k',\n",
              " 'imagetransformer_sep_channels',\n",
              " 'adaptive_universal_transformer_tiny',\n",
              " 'attention_lm_no_moe_small',\n",
              " 'attention_lm_moe_32b_diet',\n",
              " 'transformer_test',\n",
              " 'transformer_librispeech_tpu',\n",
              " 'next_frame_sv2p_cutoff',\n",
              " 'imagetransformer_base_8l_8h_big_cond_dr03_dan_128',\n",
              " 'imagetransformer_base_14l_8h_big',\n",
              " 'next_frame_tpu',\n",
              " 'aligned_8k_grouped',\n",
              " 'transformer_ada_lmpackedbase_dialog',\n",
              " 'vqa_attention_drop01_dna',\n",
              " 'imagetransformer_b10l_4h_big_uncond_dr03_tpu',\n",
              " 'imagetransformerpp_base_14l_8h_big_uncond_dr03_dan_eval',\n",
              " 'mtf_image_transformer_base',\n",
              " 'afx_adafactor',\n",
              " 'resnet_200',\n",
              " 'vqa_attention_feature_batch1024_drop01_dna_concat',\n",
              " 'attention_lm_16k',\n",
              " 'next_frame_basic_stochastic_discrete',\n",
              " 'mtf_image_transformer_length_sharded',\n",
              " 'autoencoder_discrete_cifar',\n",
              " 'transformer_tpu',\n",
              " 'imagetransformer2d_base_8l_8_16_ls',\n",
              " 'aligned_no_timing',\n",
              " 'transformer_ae_a8',\n",
              " 'lmx_h2k_f8k',\n",
              " 'transformer_ae_a6',\n",
              " 'lmx_h4k_f16k',\n",
              " 'transformer_ae_a3',\n",
              " 'attention_lm_moe_translation',\n",
              " 'afx_pow05',\n",
              " 'resnet_cifar_15',\n",
              " 'afx_pow08',\n",
              " 'attention_lm_attention_moe_tiny',\n",
              " 'cycle_gan_small',\n",
              " 'imagetransformerpp_tiny',\n",
              " 'imagetransformer_base_14l_8h_big_uncond',\n",
              " 'shakeshake_tpu',\n",
              " 'imagetransformer_base_12l_8h_big_uncond',\n",
              " 'mtf_transformer_paper_tr_4_mesh_16_8',\n",
              " 'afx_clip2',\n",
              " 'ppo_discrete_action_base',\n",
              " 'resnet_101',\n",
              " 'imagetransformerpp_base_12l_8h_big_uncond_dr03_dan_m_relsh',\n",
              " 'resnet_imagenet_34',\n",
              " 'imagetransformer_b12l_8h_b256_uncond_dr03_tpu',\n",
              " 'bytenet_base',\n",
              " 'vqa_attention_feature_batch1024_dnz',\n",
              " 'super_lm_tpu',\n",
              " 'next_frame_sv2p_atari',\n",
              " 'xmoe_dense_64k',\n",
              " 'lmx_h1k_f64k',\n",
              " 'ppo_pong_ae_base',\n",
              " 'afx_unscale',\n",
              " 'universal_transformer_sepconv_big',\n",
              " 'afx_relative',\n",
              " 'img2img_transformer2d_n3',\n",
              " 'revnet_110_cifar',\n",
              " 'attention_lm_moe_base_long_seq',\n",
              " 'lstm_luong_attention',\n",
              " 'autoencoder_ordered_discrete_patched',\n",
              " 'mtf_transformer_paper_lm_m1',\n",
              " 'aligned_pseudolocal',\n",
              " 'super_lm_b8k',\n",
              " 'attention_lm_translation',\n",
              " 'next_frame_basic_stochastic',\n",
              " 'slicenet_1noam',\n",
              " 'imagetransformer2d_base_14l_8_16_big',\n",
              " 'aligned_lsh',\n",
              " 'mtf_transformer_base',\n",
              " 'resnet_34',\n",
              " 'revnet_38_cifar',\n",
              " 'img2img_transformer2d_q1',\n",
              " 'img2img_transformer2d_q2',\n",
              " 'img2img_transformer2d_q3',\n",
              " 'attention_lm_moe_large',\n",
              " 'imagetransformer_sep_channels_8l',\n",
              " 'imagetransformer1d_base_12l_64by64',\n",
              " 'resnet_cifar_32_td_unit_no_drop',\n",
              " 'attention_lm_moe_memory_efficient',\n",
              " 'img2img_transformer_tiny_tpu',\n",
              " 'transformer_parsing_base',\n",
              " 'adaptive_universal_transformer_mix_before_ut_base',\n",
              " 'transformer_parameter_attention_b',\n",
              " 'imagetransformer_bas8l_8h_big_uncond_dr03_imgnet',\n",
              " 'transformer_parameter_attention_a',\n",
              " 'aligned_local',\n",
              " 'attention_lm_translation_l12',\n",
              " 'transformer_big_single_gpu',\n",
              " 'universal_transformer_lstm_base',\n",
              " 'transformer_ls2',\n",
              " 'transformer_ls0',\n",
              " 'super_lm_conv',\n",
              " 'super_lm_moe',\n",
              " 'vqa_attention_feature_dnz',\n",
              " 'imagetransformerpp_base_6l_8h_dr00_dan_g_bs1_adafactor',\n",
              " 'transformer_tpu_1b',\n",
              " 'xception_tiny',\n",
              " 'transformer_dr2',\n",
              " 'imagetransformer_b12l_4h_uncond_dr03_tpu',\n",
              " 'imagetransformer_sep_channels_16l_16h_imgnet_lrg_loc',\n",
              " 'adaptive_universal_transformer_tall_actlossw0',\n",
              " 'transformer_moe_prepend_8k',\n",
              " 'imagetransformer_sep_channels_8l_8h_local_and_global_att',\n",
              " 'image_transformer2d_base',\n",
              " 'afx_clip',\n",
              " 'adaptive_universal_transformer_tall_actlossw001',\n",
              " 'vqa_self_attention_feature_lr5',\n",
              " 'imagetransformer_sep_channels_16l_16h_imgnet_lrg_loc_128',\n",
              " 'transformer_prepend',\n",
              " 'transformer_lm_tpu_0',\n",
              " 'transformer_lm_tpu_1',\n",
              " 'universal_transformer_position_step_timing_tiny',\n",
              " 'vqa_self_attention_feature_batch1024_hidden6',\n",
              " 'transformer_base',\n",
              " 'vqa_attention_feature_base',\n",
              " 'imagetransformerpp_base_5l_8h_dr00_dan_g_bs1_adafactor',\n",
              " 'xmoe_dense_4k',\n",
              " 'imagetransformer_base_8l_8h_big_cond_dr03_dan_dilated',\n",
              " 'img2img_transformer_tiny',\n",
              " 'imagetransformerpp_base_12l_8h_big_uncond_dr03_dan_m_bs1',\n",
              " 'universal_transformer_gru_base',\n",
              " 'vqa_attention_base',\n",
              " 'imagetransformer_base_tpu',\n",
              " 'basic_1',\n",
              " 'universal_transformer_small_dropconnect',\n",
              " 'img2img_transformer_b1',\n",
              " 'basic_policy_parameters',\n",
              " 'attention_lm_small',\n",
              " 'img2img_transformer_b2',\n",
              " 'img2img_transformer2d_n44',\n",
              " 'autoencoder_ordered_discrete_vq',\n",
              " 'transformer_base_vq_ada_32ex_packed',\n",
              " 'mtf_image_transformer_tiny',\n",
              " 'vqa_attention_feature_numglimps1',\n",
              " 'mtf_transformer_paper_tr_0_mesh_128',\n",
              " 'imagetransformer_sep_channels_12l_16h_imagenet_large',\n",
              " 'imagetransformer2d_base_12l_8_64_64by64',\n",
              " 'transformer_base_multistep8',\n",
              " 'aligned_local_expert',\n",
              " 'xception_tiny_tpu',\n",
              " 'super_lm_moe_4b_diet',\n",
              " 'ppo_pong_base',\n",
              " 'transformer_revnet_big',\n",
              " 'transformer_clean_big_tpu',\n",
              " 'attention_lm_12k',\n",
              " 'imagetransformer2d_base_14l_8_16_big_uncond',\n",
              " 'mtf_transformer_single',\n",
              " 'transformer_parsing_ice',\n",
              " 'transformer_big_dr2',\n",
              " 'vqa_recurrent_self_attention_small',\n",
              " 'transformer_big_dr1',\n",
              " 'transformer_moe_12k',\n",
              " 'transformer_base_vq1_16_nb1_packed_nda_b01_scales',\n",
              " 'mtf_transformer_paper_tr_2',\n",
              " 'mtf_transformer_paper_tr_3',\n",
              " 'mtf_transformer_paper_tr_0',\n",
              " 'mtf_transformer_paper_tr_1',\n",
              " 'adaptive_universal_transformer_small',\n",
              " 'mtf_transformer_paper_tr_4',\n",
              " 'imagetransformer_base_10l_8h_big_uncond_dr03_dan_64_2d',\n",
              " 'next_frame_pixel_noise',\n",
              " 'afx_fast',\n",
              " 'imagetransformerpp_base_14l_8h_big_uncond_dr03_dan_p_bs1',\n",
              " 'imagetransformer2d_base_8l_8_64_64by64',\n",
              " 'vqa_self_attention_feature_batch1024_drop03',\n",
              " 'transformer_dr0',\n",
              " 'img2img_transformer_base',\n",
              " 'transformer_topk_16_packed',\n",
              " 'vqa_attention_feature_imagefeat512',\n",
              " 'resnet_cifar_32_td_unit_05_05',\n",
              " 'imagetransformer_tiny',\n",
              " 'imagetransformer_b12l_4h_b256_uncond_dr03_tpu',\n",
              " 'universal_transformer_tiny',\n",
              " 'imagetransformer_base_10l_16h_big_dr01_moe_imgnet',\n",
              " 'afx_pow10',\n",
              " 'xmoe_2d',\n",
              " 'transformer_symshard_h4',\n",
              " 'attention_lm_moe_small',\n",
              " 'super_lm_big',\n",
              " 'next_frame_sv2p',\n",
              " 'next_frame_sv2p_tiny',\n",
              " 'imagetransformerpp_base_8l_8h_big_cond_dr03_dan']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1DtfzgqivAxl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "6d416d85-0b1e-45fd-dd08-bc0b19b45f75"
      },
      "cell_type": "code",
      "source": [
        "from tensor2tensor.utils.trainer_lib import create_run_config, create_experiment, create_hparams\n",
        "\n",
        "# Setup and create directories.\n",
        "DATA_DIR = os.path.expanduser(\"/tmp/t2t/data\")\n",
        "OUTPUT_DIR = os.path.expanduser(\"/tmp/t2t/output\")\n",
        "TMP_DIR = os.path.expanduser(\"/tmp/t2t/tmp\")\n",
        "\n",
        "hparams = create_hparams('transformer_base_single_gpu')\n",
        "hparams.batch_size = 1024\n",
        "hparams.learning_rate_warmup_steps = 45000\n",
        "hparams.learning_rate = .4\n",
        "\n",
        "RUN_CONFIG = create_run_config(\n",
        "      model_dir=OUTPUT_DIR,\n",
        "      keep_checkpoint_max=3\n",
        ")\n",
        "\n",
        "exp_fn = create_experiment(\n",
        "    run_config = RUN_CONFIG,\n",
        "    hparams=hparams,\n",
        "    model_name='transformer',\n",
        "    problem_name='translate_entn_rma',\n",
        "    data_dir=DATA_DIR,\n",
        "    train_steps=125000,\n",
        "    eval_steps=100\n",
        ")\n",
        "exp_fn.train_and_evaluate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_lib.py:198: __init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
            "INFO:tensorflow:schedule=continuous_train_and_eval\n",
            "INFO:tensorflow:worker_gpu=1\n",
            "INFO:tensorflow:sync=False\n",
            "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
            "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
            "INFO:tensorflow:caching_devices: None\n",
            "INFO:tensorflow:ps_devices: ['gpu:0']\n",
            "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 3, '_task_type': None, '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb909728690>, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_protocol': None, '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 10000, '_session_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 0.95\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  optimizer_options {\n",
            "  }\n",
            "}\n",
            ", '_model_dir': '/tmp/t2t/output', 'use_tpu': False, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fb9097286d0>, '_environment': 'local', '_save_summary_steps': 100, 't2t_device_info': {'num_async_replicas': 1}}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7fb90847ef50>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Using ValidationMonitor\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:279: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
            "Instructions for updating:\n",
            "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
            "WARNING:tensorflow:EvalSpec not provided. Estimator will not manage model evaluation. Assuming ValidationMonitor present in train_hooks.\n",
            "INFO:tensorflow:Reading data files from /tmp/t2t/data/translate_entn_rma-train*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 100\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_27854_512.bottom\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/function.py:988: calling create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
            "INFO:tensorflow:Transforming 'targets' with symbol_modality_27854_512.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_27854_512.top\n",
            "INFO:tensorflow:Base learning rate: 2.000000\n",
            "INFO:tensorflow:Trainable Variables Total size: 58381312\n",
            "INFO:tensorflow:Using optimizer Adam\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/t2t/output/model.ckpt.\n",
            "INFO:tensorflow:loss = 9.278147, step = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xRtfC0sHBlSo"
      },
      "cell_type": "markdown",
      "source": [
        "## To be continued ...\n",
        "\n",
        "Stay tuned for additions to this notebook for adding problems with non-text modalities like Images, Audio and Video!"
      ]
    }
  ]
}