{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fairseq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rbZJFNEOlsgi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installing Pytorch"
      ]
    },
    {
      "metadata": {
        "id": "FQTz09F_l5Ry",
        "colab_type": "code",
        "outputId": "5f04d738-927c-4428-a2e2-422d77659b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5919e000 @  0x7f427a7b62a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D2uXn6IgmI_h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building Fairseq"
      ]
    },
    {
      "metadata": {
        "id": "u8DXFDikmIqK",
        "colab_type": "code",
        "outputId": "fd806705-c6ba-4b75-8696-c99ce0e24207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/fairseq.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 3094 (delta 13), reused 8 (delta 7), pack-reused 3063\u001b[K\n",
            "Receiving objects: 100% (3094/3094), 3.14 MiB | 3.32 MiB/s, done.\n",
            "Resolving deltas: 100% (2247/2247), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jKcMF36Vo8xl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"fairseq/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rey0HHj3n2Lf",
        "colab_type": "code",
        "outputId": "749843d4-721a-46ef-d7e1-842f0aaf0f2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2910
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "%run -i 'setup.py' build develop"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.11.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.28.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->-r requirements.txt (line 1)) (2.19)\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_iterators.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_noising.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_convtbc.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_dictionary.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_character_token_embedder.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_average_checkpoints.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_sequence_scorer.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_label_smoothing.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_utils.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_binaries.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/__init__.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_backtranslation_dataset.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_sequence_generator.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_reproducibility.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/utils.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_train.py -> build/lib.linux-x86_64-3.6/tests\n",
            "creating build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/progress_bar.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/distributed_utils.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/bleu.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/sequence_generator.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/multiprocessing_pdb.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/legacy_distributed_data_parallel.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/trainer.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/sequence_scorer.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/search.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/meters.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/__init__.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/options.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/utils.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "creating build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/average_checkpoints.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/build_sym_alignment.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/__init__.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/read_binarized.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/adagrad.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/fairseq_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/sgd.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/adam.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/nag.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/scalar_bias.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/grad_multiply.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/conv_tbc.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/beamable_mm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/character_token_embedder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/highway.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/linearized_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/learned_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/sinusoidal_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/adaptive_softmax.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/adaptive_input.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/downsampled_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/multilingual_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fconv_self_att.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/lstm.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_incremental_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/distributed_fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/composite_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/language_modeling.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/fairseq_task.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/multilingual_translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/transform_eos_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/fairseq_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/language_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/backtranslation_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/noising.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/indexed_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/monolingual_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/iterators.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/concat_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/token_block_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/round_robin_zip_datasets.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/adaptive_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/composite_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/label_smoothed_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/fairseq_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/cosine_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/fixed_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/triangular_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/inverse_square_root_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "running build_ext\n",
            "building 'fairseq.libbleu' extension\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/fairseq\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib/libbleu\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o -std=c++11\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so\n",
            "running develop\n",
            "running egg_info\n",
            "creating fairseq.egg-info\n",
            "writing fairseq.egg-info/PKG-INFO\n",
            "writing dependency_links to fairseq.egg-info/dependency_links.txt\n",
            "writing requirements to fairseq.egg-info/requires.txt\n",
            "writing top-level names to fairseq.egg-info/top_level.txt\n",
            "writing manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "reading manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "writing manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so -> fairseq\n",
            "Creating /usr/local/lib/python3.6/dist-packages/fairseq.egg-link (link to .)\n",
            "Adding fairseq 0.6.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/fairseq\n",
            "Processing dependencies for fairseq==0.6.0\n",
            "Searching for tqdm==4.28.1\n",
            "Best match: tqdm 4.28.1\n",
            "Adding tqdm 4.28.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for torch==0.4.1\n",
            "Best match: torch 0.4.1\n",
            "Adding torch 0.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.14.6\n",
            "Best match: numpy 1.14.6\n",
            "Adding numpy 1.14.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cffi==1.11.5\n",
            "Best match: cffi 1.11.5\n",
            "Adding cffi 1.11.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pycparser==2.19\n",
            "Best match: pycparser 2.19\n",
            "Adding pycparser 2.19 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for fairseq==0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvMtXVjuBaK4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model for Afrikaans\n",
        "### Data Preprop\n",
        "Upload the data in the form train.en, train.tn, valid.en, valid.tn, test.en, test.tn."
      ]
    },
    {
      "metadata": {
        "id": "uv7RW7fR9Yeg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Subword"
      ]
    },
    {
      "metadata": {
        "id": "hALHCsup9X52",
        "colab_type": "code",
        "outputId": "69710140-6337-4db0-8e9e-253d2b94d028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rsennrich/subword-nmt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects:  25% (1/4)   \u001b[K\rremote: Counting objects:  50% (2/4)   \u001b[K\rremote: Counting objects:  75% (3/4)   \u001b[K\rremote: Counting objects: 100% (4/4)   \u001b[K\rremote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 485 (delta 0), reused 1 (delta 0), pack-reused 481\u001b[K\n",
            "Receiving objects: 100% (485/485), 205.64 KiB | 388.00 KiB/s, done.\n",
            "Resolving deltas: 100% (287/287), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wHpzQrY59dIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOqfutkh9U6P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "# Learn a vocabulary using 40,000 merge operations\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 4000 <combine.txt> enaf.codes\n",
        "\n",
        "# Apply the vocabulary to the training file\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgN47Lks-MYt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('fairseq/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvH9D2RQBdiZ",
        "colab_type": "code",
        "outputId": "cb84604a-1e7f-4f01-eef1-b3690a865e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(alignfile=None, destdir='data-bin/enaf', joined_dictionary=False, nwordssrc=-1, nwordstgt=-1, only_source=False, output_format='binary', padding_factor=8, source_lang='en', srcdict=None, target_lang='af', testpref='../test', tgtdict=None, thresholdsrc=0, thresholdtgt=0, trainpref='../train', validpref='../valid', workers=1)\n",
            "| [en] Dictionary: 3511 types\n",
            "| [en] ../train.en: 37219 sents, 790613 tokens, 0.0% replaced by <unk>\n",
            "| [en] Dictionary: 3511 types\n",
            "| [en] ../valid.en: 12953 sents, 383339 tokens, 0.114% replaced by <unk>\n",
            "| [en] Dictionary: 3511 types\n",
            "| [en] ../test.en: 3000 sents, 76516 tokens, 0.0562% replaced by <unk>\n",
            "| [af] Dictionary: 3967 types\n",
            "| [af] ../train.af: 37219 sents, 813055 tokens, 0.0% replaced by <unk>\n",
            "| [af] Dictionary: 3967 types\n",
            "| [af] ../valid.af: 12953 sents, 403141 tokens, 0.344% replaced by <unk>\n",
            "| [af] Dictionary: 3967 types\n",
            "| [af] ../test.af: 3000 sents, 78473 tokens, 0.0624% replaced by <unk>\n",
            "| Wrote preprocessed data to data-bin/enaf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lsk8qqhmBjSw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ]
    },
    {
      "metadata": {
        "id": "z7zy0QquBmsg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p chckpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir ckpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path ckpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z74u-7RbBs6o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "Generate translations from the test data. Calculate the Bleu score."
      ]
    },
    {
      "metadata": {
        "id": "sAGRGvz2Bv5m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path ckpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8HW8jAKyXvG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**8k**"
      ]
    },
    {
      "metadata": {
        "id": "qosFjU2fyY_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 8000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir checkpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path checkpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHwS-r_UyZh1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**12k**"
      ]
    },
    {
      "metadata": {
        "id": "3p_Vap6Wya29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 12000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir ceckpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path ceckpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYnfAuBpybUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**16k**"
      ]
    },
    {
      "metadata": {
        "id": "nqrE4wDSydDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 16000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir chekpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path chekpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2HTIUWVFydem",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**20k**"
      ]
    },
    {
      "metadata": {
        "id": "UmN6Roccye1e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 20000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir chckpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path chckpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cfq9EL8CyfWw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**24k**"
      ]
    },
    {
      "metadata": {
        "id": "aVHz8s4Lygnm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 24000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir checpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path checpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mt9VPBPqyhIt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**28k**"
      ]
    },
    {
      "metadata": {
        "id": "DCHKH53s_tfp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 28000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir checpin/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path checpin/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "321XJhen_xtq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**32k**"
      ]
    },
    {
      "metadata": {
        "id": "ChBPsrYQ_z_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 32000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir checpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path checpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t5LX6mOzRhz2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**36k**"
      ]
    },
    {
      "metadata": {
        "id": "Qs3FqDYvRk0M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 36000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir checpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path checpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLbt1AO0RjdU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**40k**"
      ]
    },
    {
      "metadata": {
        "id": "SVKZuJ5KRlRs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 40000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir chepoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path chepoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r9LE_5D8X8Zc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "*   40k: 19.00\n",
        "*   36k: 16.28\n",
        "*   32k: 20.61\n",
        "*   28k: 20.91\n",
        "*   24k: 22.80\n",
        "*   20k: 13.46\n",
        "*   16k: 23.06\n",
        "*   12k: 20.88\n",
        "*     8k: 23.20\n",
        "*     4k: 25.04\n",
        "\n"
      ]
    }
  ]
}