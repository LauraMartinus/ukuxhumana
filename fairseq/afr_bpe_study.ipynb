{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fairseq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rbZJFNEOlsgi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installing Pytorch"
      ]
    },
    {
      "metadata": {
        "id": "FQTz09F_l5Ry",
        "colab_type": "code",
        "outputId": "ba781ecc-1e2c-4449-9d93-9184aca3d543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5789c000 @  0x7f752ca512a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D2uXn6IgmI_h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building Fairseq"
      ]
    },
    {
      "metadata": {
        "id": "u8DXFDikmIqK",
        "colab_type": "code",
        "outputId": "e63c5fce-cf87-4127-d7dd-b9767768f9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/fairseq.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 2992 (delta 45), reused 47 (delta 27), pack-reused 2899\u001b[K\n",
            "Receiving objects: 100% (2992/2992), 3.08 MiB | 5.24 MiB/s, done.\n",
            "Resolving deltas: 100% (2171/2171), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jKcMF36Vo8xl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"fairseq/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rey0HHj3n2Lf",
        "colab_type": "code",
        "outputId": "47355c02-0c32-4f35-8b43-30dab7843dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3078
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "%run -i 'setup.py' build develop"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cffi (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/c0/47db8f624f3e4e2f3f27be03a93379d1ba16a1450a7b1aacfa0366e2c0dd/cffi-1.11.5-cp36-cp36m-manylinux1_x86_64.whl (421kB)\n",
            "\r\u001b[K    2% |▊                               | 10kB 22.1MB/s eta 0:00:01\r\u001b[K    4% |█▌                              | 20kB 4.6MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 30kB 6.5MB/s eta 0:00:01\r\u001b[K    9% |███                             | 40kB 4.2MB/s eta 0:00:01\r\u001b[K    12% |███▉                            | 51kB 5.1MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 61kB 6.0MB/s eta 0:00:01\r\u001b[K    17% |█████▍                          | 71kB 6.8MB/s eta 0:00:01\r\u001b[K    19% |██████▏                         | 81kB 7.6MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 92kB 6.1MB/s eta 0:00:01\r\u001b[K    24% |███████▊                        | 102kB 6.7MB/s eta 0:00:01\r\u001b[K    26% |████████▌                       | 112kB 6.8MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 122kB 9.0MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 133kB 9.0MB/s eta 0:00:01\r\u001b[K    34% |██████████▉                     | 143kB 15.7MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 153kB 16.0MB/s eta 0:00:01\r\u001b[K    38% |████████████▍                   | 163kB 16.0MB/s eta 0:00:01\r\u001b[K    41% |█████████████▏                  | 174kB 15.8MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 184kB 15.9MB/s eta 0:00:01\r\u001b[K    46% |██████████████▊                 | 194kB 45.6MB/s eta 0:00:01\r\u001b[K    48% |███████████████▌                | 204kB 44.1MB/s eta 0:00:01\r\u001b[K    51% |████████████████▎               | 215kB 20.2MB/s eta 0:00:01\r\u001b[K    53% |█████████████████               | 225kB 20.3MB/s eta 0:00:01\r\u001b[K    55% |█████████████████▉              | 235kB 20.6MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 245kB 20.5MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▍            | 256kB 20.5MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▏           | 266kB 18.9MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 276kB 19.3MB/s eta 0:00:01\r\u001b[K    68% |█████████████████████▊          | 286kB 19.4MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▌         | 296kB 19.3MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 307kB 20.0MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████        | 317kB 44.5MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 327kB 21.0MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▋      | 337kB 21.3MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▍     | 348kB 20.2MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 358kB 20.2MB/s eta 0:00:01\r\u001b[K    87% |████████████████████████████    | 368kB 22.0MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▊   | 378kB 22.1MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 389kB 22.0MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▎ | 399kB 22.0MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████ | 409kB 21.9MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 419kB 22.0MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 430kB 25.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.28.1)\n",
            "Collecting pycparser (from cffi->-r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\n",
            "\r\u001b[K    6% |██                              | 10kB 30.8MB/s eta 0:00:01\r\u001b[K    12% |████▏                           | 20kB 34.5MB/s eta 0:00:01\r\u001b[K    19% |██████▏                         | 30kB 39.4MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 40kB 41.3MB/s eta 0:00:01\r\u001b[K    32% |██████████▍                     | 51kB 41.3MB/s eta 0:00:01\r\u001b[K    38% |████████████▍                   | 61kB 43.1MB/s eta 0:00:01\r\u001b[K    45% |██████████████▌                 | 71kB 43.4MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 81kB 43.9MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 92kB 45.3MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▊           | 102kB 46.1MB/s eta 0:00:01\r\u001b[K    71% |██████████████████████▊         | 112kB 49.6MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 122kB 50.7MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████     | 133kB 48.6MB/s eta 0:00:01\r\u001b[K    90% |█████████████████████████████   | 143kB 49.9MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████ | 153kB 51.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 163kB 31.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pycparser\n",
            "  Running setup.py bdist_wheel for pycparser ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511\n",
            "Successfully built pycparser\n",
            "Installing collected packages: pycparser, cffi\n",
            "Successfully installed cffi-1.11.5 pycparser-2.19\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_train.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_binaries.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/utils.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_sequence_scorer.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/__init__.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_noising.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_label_smoothing.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_average_checkpoints.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_iterators.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_convtbc.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_dictionary.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_character_token_embedder.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_utils.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_reproducibility.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_backtranslation_dataset.py -> build/lib.linux-x86_64-3.6/tests\n",
            "copying tests/test_sequence_generator.py -> build/lib.linux-x86_64-3.6/tests\n",
            "creating build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/sequence_scorer.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/progress_bar.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/meters.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/utils.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/__init__.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/distributed_utils.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/options.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/bleu.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/sequence_generator.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/trainer.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/multiprocessing_pdb.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/legacy_distributed_data_parallel.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "copying fairseq/search.py -> build/lib.linux-x86_64-3.6/fairseq\n",
            "creating build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/average_checkpoints.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/read_binarized.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/__init__.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "copying scripts/build_sym_alignment.py -> build/lib.linux-x86_64-3.6/scripts\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/adaptive_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/label_smoothed_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/fairseq_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "copying fairseq/criterions/composite_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/nag.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/adagrad.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/sgd.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/fairseq_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "copying fairseq/optim/adam.py -> build/lib.linux-x86_64-3.6/fairseq/optim\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/language_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/append_eos_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/indexed_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/token_block_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/noising.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/backtranslation_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/iterators.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/fairseq_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/monolingual_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/concat_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "copying fairseq/data/round_robin_zip_datasets.py -> build/lib.linux-x86_64-3.6/fairseq/data\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/multilingual_translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/fairseq_task.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "copying fairseq/tasks/language_modeling.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/sinusoidal_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/grad_multiply.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/adaptive_input.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/downsampled_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/scalar_bias.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/beamable_mm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/linearized_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/conv_tbc.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/highway.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/learned_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/character_token_embedder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "copying fairseq/modules/adaptive_softmax.py -> build/lib.linux-x86_64-3.6/fairseq/modules\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/multilingual_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/composite_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fconv_self_att.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/lstm.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/fairseq_incremental_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "copying fairseq/models/distributed_fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\n",
            "creating build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/fixed_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/cosine_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/inverse_square_root_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/triangular_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\n",
            "running build_ext\n",
            "building 'fairseq.libbleu' extension\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/fairseq\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib\n",
            "creating build/temp.linux-x86_64-3.6/fairseq/clib/libbleu\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o -std=c++11\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so\n",
            "running develop\n",
            "running egg_info\n",
            "creating fairseq.egg-info\n",
            "writing fairseq.egg-info/PKG-INFO\n",
            "writing dependency_links to fairseq.egg-info/dependency_links.txt\n",
            "writing requirements to fairseq.egg-info/requires.txt\n",
            "writing top-level names to fairseq.egg-info/top_level.txt\n",
            "writing manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "reading manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "writing manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so -> fairseq\n",
            "Creating /usr/local/lib/python3.6/dist-packages/fairseq.egg-link (link to .)\n",
            "Adding fairseq 0.6.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/fairseq\n",
            "Processing dependencies for fairseq==0.6.0\n",
            "Searching for tqdm==4.28.1\n",
            "Best match: tqdm 4.28.1\n",
            "Adding tqdm 4.28.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for torch==0.4.1\n",
            "Best match: torch 0.4.1\n",
            "Adding torch 0.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.14.6\n",
            "Best match: numpy 1.14.6\n",
            "Adding numpy 1.14.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cffi==1.11.5\n",
            "Best match: cffi 1.11.5\n",
            "Adding cffi 1.11.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pycparser==2.19\n",
            "Best match: pycparser 2.19\n",
            "Adding pycparser 2.19 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for fairseq==0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvMtXVjuBaK4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model for Afrikaans\n",
        "### Data Preprop\n",
        "Upload the data in the form train.en, train.tn, valid.en, valid.tn, test.en, test.tn."
      ]
    },
    {
      "metadata": {
        "id": "uv7RW7fR9Yeg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Subword"
      ]
    },
    {
      "metadata": {
        "id": "hALHCsup9X52",
        "colab_type": "code",
        "outputId": "8759eb1b-f2b4-49e2-92be-01b523282958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rsennrich/subword-nmt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 481, done.\u001b[K\n",
            "Receiving objects:   0% (1/481)   \rReceiving objects:   1% (5/481)   \rReceiving objects:   2% (10/481)   \rReceiving objects:   3% (15/481)   \rReceiving objects:   4% (20/481)   \rReceiving objects:   5% (25/481)   \rReceiving objects:   6% (29/481)   \rReceiving objects:   7% (34/481)   \rReceiving objects:   8% (39/481)   \rReceiving objects:   9% (44/481)   \rReceiving objects:  10% (49/481)   \rReceiving objects:  11% (53/481)   \rReceiving objects:  12% (58/481)   \rReceiving objects:  13% (63/481)   \rReceiving objects:  14% (68/481)   \rReceiving objects:  15% (73/481)   \rReceiving objects:  16% (77/481)   \rReceiving objects:  17% (82/481)   \rReceiving objects:  18% (87/481)   \rReceiving objects:  19% (92/481)   \rReceiving objects:  20% (97/481)   \rReceiving objects:  21% (102/481)   \rReceiving objects:  22% (106/481)   \rReceiving objects:  23% (111/481)   \rReceiving objects:  24% (116/481)   \rReceiving objects:  25% (121/481)   \rReceiving objects:  26% (126/481)   \rReceiving objects:  27% (130/481)   \rReceiving objects:  28% (135/481)   \rReceiving objects:  29% (140/481)   \rReceiving objects:  30% (145/481)   \rReceiving objects:  31% (150/481)   \rReceiving objects:  32% (154/481)   \rReceiving objects:  33% (159/481)   \rReceiving objects:  34% (164/481)   \rReceiving objects:  35% (169/481)   \rReceiving objects:  36% (174/481)   \rReceiving objects:  37% (178/481)   \rReceiving objects:  38% (183/481)   \rReceiving objects:  39% (188/481)   \rReceiving objects:  40% (193/481)   \rReceiving objects:  41% (198/481)   \rReceiving objects:  42% (203/481)   \rReceiving objects:  43% (207/481)   \rReceiving objects:  44% (212/481)   \rReceiving objects:  45% (217/481)   \rReceiving objects:  46% (222/481)   \rReceiving objects:  47% (227/481)   \rReceiving objects:  48% (231/481)   \rReceiving objects:  49% (236/481)   \rReceiving objects:  50% (241/481)   \rReceiving objects:  51% (246/481)   \rReceiving objects:  52% (251/481)   \rReceiving objects:  53% (255/481)   \rReceiving objects:  54% (260/481)   \rReceiving objects:  55% (265/481)   \rReceiving objects:  56% (270/481)   \rReceiving objects:  57% (275/481)   \rReceiving objects:  58% (279/481)   \rReceiving objects:  59% (284/481)   \rReceiving objects:  60% (289/481)   \rReceiving objects:  61% (294/481)   \rReceiving objects:  62% (299/481)   \rReceiving objects:  63% (304/481)   \rReceiving objects:  64% (308/481)   \rReceiving objects:  65% (313/481)   \rReceiving objects:  66% (318/481)   \rReceiving objects:  67% (323/481)   \rReceiving objects:  68% (328/481)   \rReceiving objects:  69% (332/481)   \rReceiving objects:  70% (337/481)   \rReceiving objects:  71% (342/481)   \rReceiving objects:  72% (347/481)   \rReceiving objects:  73% (352/481)   \rReceiving objects:  74% (356/481)   \rReceiving objects:  75% (361/481)   \rReceiving objects:  76% (366/481)   \rReceiving objects:  77% (371/481)   \rReceiving objects:  78% (376/481)   \rReceiving objects:  79% (380/481)   \rReceiving objects:  80% (385/481)   \rReceiving objects:  81% (390/481)   \rReceiving objects:  82% (395/481)   \rremote: Total 481 (delta 0), reused 0 (delta 0), pack-reused 481\u001b[K\n",
            "Receiving objects:  83% (400/481)   \rReceiving objects:  84% (405/481)   \rReceiving objects:  85% (409/481)   \rReceiving objects:  86% (414/481)   \rReceiving objects:  87% (419/481)   \rReceiving objects:  88% (424/481)   \rReceiving objects:  89% (429/481)   \rReceiving objects:  90% (433/481)   \rReceiving objects:  91% (438/481)   \rReceiving objects:  92% (443/481)   \rReceiving objects:  93% (448/481)   \rReceiving objects:  94% (453/481)   \rReceiving objects:  95% (457/481)   \rReceiving objects:  96% (462/481)   \rReceiving objects:  97% (467/481)   \rReceiving objects:  98% (472/481)   \rReceiving objects:  99% (477/481)   \rReceiving objects: 100% (481/481)   \rReceiving objects: 100% (481/481), 203.75 KiB | 560.00 KiB/s, done.\n",
            "Resolving deltas:   0% (0/287)   \rResolving deltas:   3% (10/287)   \rResolving deltas:  11% (34/287)   \rResolving deltas:  12% (35/287)   \rResolving deltas:  15% (45/287)   \rResolving deltas:  19% (55/287)   \rResolving deltas:  20% (60/287)   \rResolving deltas:  25% (72/287)   \rResolving deltas:  30% (88/287)   \rResolving deltas:  32% (92/287)   \rResolving deltas:  36% (105/287)   \rResolving deltas:  37% (107/287)   \rResolving deltas:  40% (116/287)   \rResolving deltas:  45% (131/287)   \rResolving deltas:  47% (135/287)   \rResolving deltas:  48% (140/287)   \rResolving deltas:  51% (147/287)   \rResolving deltas:  52% (151/287)   \rResolving deltas:  53% (153/287)   \rResolving deltas:  54% (156/287)   \rResolving deltas:  62% (178/287)   \rResolving deltas:  63% (182/287)   \rResolving deltas:  67% (195/287)   \rResolving deltas:  73% (210/287)   \rResolving deltas:  74% (215/287)   \rResolving deltas:  75% (216/287)   \rResolving deltas:  77% (223/287)   \rResolving deltas:  79% (229/287)   \rResolving deltas:  83% (239/287)   \rResolving deltas:  88% (255/287)   \rResolving deltas:  89% (258/287)   \rResolving deltas:  93% (267/287)   \rResolving deltas:  98% (282/287)   \rResolving deltas: 100% (287/287)   \rResolving deltas: 100% (287/287), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wHpzQrY59dIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOqfutkh9U6P",
        "colab_type": "code",
        "outputId": "4ed66dd9-b1e0-4246-ccc0-2cce48625f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "cell_type": "code",
      "source": [
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "# Learn a vocabulary using 40,000 merge operations\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 4000 <combine.txt> enaf.codes\n",
        "\n",
        "# Apply the vocabulary to the training file\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fairseq/subword-nmt/learn_bpe.py:267: DeprecationWarning: this script's location has moved to /content/fairseq/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "fairseq/subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "fairseq/subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='enaf.codes' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "fairseq/subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "fairseq/subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='enaf.codes' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "fairseq/subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "fairseq/subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='enaf.codes' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "fairseq/subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "fairseq/subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='enaf.codes' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "fairseq/subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "fairseq/subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='enaf.codes' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "fairseq/subword-nmt/apply_bpe.py:334: DeprecationWarning: this script's location has moved to /content/fairseq/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "fairseq/subword-nmt/apply_bpe.py:351: ResourceWarning: unclosed file <_io.TextIOWrapper name='enaf.codes' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zgN47Lks-MYt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('fairseq/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvH9D2RQBdiZ",
        "colab_type": "code",
        "outputId": "7b7c11cb-f82e-4003-e82d-8144ae6a673b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(alignfile=None, destdir='data-bin/enaf', joined_dictionary=False, nwordssrc=-1, nwordstgt=-1, only_source=False, output_format='binary', padding_factor=8, source_lang='en', srcdict=None, target_lang='af', testpref='../test', tgtdict=None, thresholdsrc=0, thresholdtgt=0, trainpref='../train', validpref='../valid', workers=1)\n",
            "| [en] Dictionary: 3511 types\n",
            "| [en] ../train.en: 37219 sents, 790613 tokens, 0.0% replaced by <unk>\n",
            "| [en] Dictionary: 3511 types\n",
            "| [en] ../valid.en: 12953 sents, 383339 tokens, 0.114% replaced by <unk>\n",
            "| [en] Dictionary: 3511 types\n",
            "| [en] ../test.en: 3000 sents, 76516 tokens, 0.0562% replaced by <unk>\n",
            "| [af] Dictionary: 3967 types\n",
            "| [af] ../train.af: 37219 sents, 813055 tokens, 0.0% replaced by <unk>\n",
            "| [af] Dictionary: 3967 types\n",
            "| [af] ../valid.af: 12953 sents, 403141 tokens, 0.344% replaced by <unk>\n",
            "| [af] Dictionary: 3967 types\n",
            "| [af] ../test.af: 3000 sents, 78473 tokens, 0.0624% replaced by <unk>\n",
            "| Wrote preprocessed data to data-bin/enaf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lsk8qqhmBjSw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ]
    },
    {
      "metadata": {
        "id": "z7zy0QquBmsg",
        "colab_type": "code",
        "outputId": "bafe9c3b-8503-4ab5-9875-90b84f6810db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p chckpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir ckpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path ckpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(arch='fconv', bucket_cap_mb=150, clip_norm=0.1, criterion='cross_entropy', data=['data-bin/enaf'], ddp_backend='c10d', decoder_attention='True', decoder_embed_dim=512, decoder_embed_path=None, decoder_layers='[(512, 3)] * 20', decoder_out_embed_dim=256, device_id=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, encoder_embed_dim=512, encoder_embed_path=None, encoder_layers='[(512, 3)] * 20', fix_batches_to_gpus=False, fp16=False, fp16_init_scale=128, fp16_scale_window=None, keep_interval_updates=-1, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.25], lr_scheduler='reduce_lr_on_plateau', lr_shrink=0.1, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_update=0, min_loss_scale=0.0001, min_lr=1e-05, momentum=0.99, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, optimizer='nag', optimizer_overrides='{}', raw_text=False, reset_lr_scheduler=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='ckpoint/fconv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', train_subset='train', update_freq=[1], upsample_primary=1, valid_subset='valid', validate_interval=1, weight_decay=0.0)\n",
            "| [en] dictionary: 3512 types\n",
            "| [af] dictionary: 3968 types\n",
            "| data-bin/enaf train 37219 examples\n",
            "| data-bin/enaf valid 12953 examples\n",
            "| model fconv, criterion CrossEntropyCriterion\n",
            "| num. model params: 80346368\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = 4000 and max sentences per GPU = None\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "| epoch 001 | loss 9.723 | ppl 845.17 | wps 2841 | ups 0.9 | wpb 3103 | bsz 142 | num_updates 262 | lr 0.25 | gnorm 1.260 | clip 100% | oom 0 | wall 288 | train_wall 281\n",
            "| epoch 001 | valid on 'valid' subset | valid_loss 9.16714 | valid_ppl 574.89 | num_updates 262\n",
            "| epoch 002 | loss 8.429 | ppl 344.73 | wps 2830 | ups 0.8 | wpb 3103 | bsz 142 | num_updates 524 | lr 0.25 | gnorm 15.921 | clip 100% | oom 0 | wall 636 | train_wall 563\n",
            "| epoch 002 | valid on 'valid' subset | valid_loss 8.5595 | valid_ppl 377.28 | num_updates 524 | best 8.5595\n",
            "| epoch 003 | loss 7.512 | ppl 182.48 | wps 2830 | ups 0.8 | wpb 3103 | bsz 142 | num_updates 786 | lr 0.25 | gnorm 334.698 | clip 100% | oom 0 | wall 983 | train_wall 845\n",
            "| epoch 003 | valid on 'valid' subset | valid_loss 7.88447 | valid_ppl 236.30 | num_updates 786 | best 7.88447\n",
            "| epoch 004 | loss 6.596 | ppl 96.74 | wps 2825 | ups 0.8 | wpb 3103 | bsz 142 | num_updates 1048 | lr 0.25 | gnorm 491.455 | clip 100% | oom 0 | wall 1330 | train_wall 1128\n",
            "| epoch 004 | valid on 'valid' subset | valid_loss 7.21621 | valid_ppl 148.69 | num_updates 1048 | best 7.21621\n",
            "| epoch 005 | loss 5.789 | ppl 55.31 | wps 2829 | ups 0.8 | wpb 3103 | bsz 142 | num_updates 1310 | lr 0.25 | gnorm 927.749 | clip 100% | oom 0 | wall 1678 | train_wall 1410\n",
            "| epoch 005 | valid on 'valid' subset | valid_loss 6.7243 | valid_ppl 105.73 | num_updates 1310 | best 6.7243\n",
            "| epoch 006 | loss 5.182 | ppl 36.30 | wps 2825 | ups 0.8 | wpb 3103 | bsz 142 | num_updates 1572 | lr 0.25 | gnorm 650.210 | clip 100% | oom 0 | wall 2025 | train_wall 1692\n",
            "| epoch 006 | valid on 'valid' subset | valid_loss 6.3656 | valid_ppl 82.46 | num_updates 1572 | best 6.3656\n",
            "| epoch 007 | loss 4.739 | ppl 26.70 | wps 2831 | ups 0.8 | wpb 3103 | bsz 142 | num_updates 1834 | lr 0.25 | gnorm 2299.686 | clip 100% | oom 0 | wall 2373 | train_wall 1975\n",
            "| epoch 007 | valid on 'valid' subset | valid_loss 5.99325 | valid_ppl 63.70 | num_updates 1834 | best 5.99325\n",
            "| epoch 008 | loss 4.364 | ppl 20.59 | wps 2828 | ups 0.8 | wpb 3103 | bsz 142 | num_updates 2096 | lr 0.25 | gnorm 3436.873 | clip 100% | oom 0 | wall 2720 | train_wall 2257\n",
            "| epoch 008 | valid on 'valid' subset | valid_loss 5.71906 | valid_ppl 52.68 | num_updates 2096 | best 5.71906\n",
            "| epoch 009 | loss 4.077 | ppl 16.88 | wps 2827 | ups 0.8 | wpb 3103 | bsz 142 | num_updates 2358 | lr 0.25 | gnorm 286.987 | clip 100% | oom 0 | wall 3068 | train_wall 2540\n",
            "| epoch 009 | valid on 'valid' subset | valid_loss 5.52284 | valid_ppl 45.98 | num_updates 2358 | best 5.52284\n",
            "| epoch 010:  64% 167/262 [03:02<01:44,  1.10s/it, loss=3.846, ppl=14.38, wps=2780, ups=0.7, wpb=3034, bsz=142, num_updates=2525, lr=0.25, gnorm=895.895, clip=100%, oom=0, wall=3310, train_wall=2719]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z74u-7RbBs6o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "Generate translations from the test data. Calculate the Bleu score."
      ]
    },
    {
      "metadata": {
        "id": "sAGRGvz2Bv5m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path ckpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8HW8jAKyXvG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**8k**"
      ]
    },
    {
      "metadata": {
        "id": "qosFjU2fyY_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 8000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir checkpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path checkpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHwS-r_UyZh1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**12k**"
      ]
    },
    {
      "metadata": {
        "id": "3p_Vap6Wya29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 12000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir ceckpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path ceckpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYnfAuBpybUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**16k**"
      ]
    },
    {
      "metadata": {
        "id": "nqrE4wDSydDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 16000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir chekpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path chekpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2HTIUWVFydem",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**20k**"
      ]
    },
    {
      "metadata": {
        "id": "UmN6Roccye1e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 20000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir chckpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path chckpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cfq9EL8CyfWw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**24k**"
      ]
    },
    {
      "metadata": {
        "id": "aVHz8s4Lygnm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!cat enaf_parallel.train.en enaf_parallel.train.af > combine.txt\n",
        "\n",
        "!fairseq/subword-nmt/learn_bpe.py -s 24000 <combine.txt> enaf.codes\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.en > train.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.train.af > train.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.en > valid.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.dev.af > valid.af\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.en > test.en\n",
        "!fairseq/subword-nmt/apply_bpe.py -c enaf.codes < enaf_parallel.test.af > test.af\n",
        "\n",
        "os.chdir('fairseq/')\n",
        "TEXT=\"..\"\n",
        "%run 'preprocess.py' --source-lang en --target-lang af --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/enaf\n",
        "\n",
        "!mkdir -p checkpoint/fconv\n",
        "!python train.py data-bin/enaf \\\n",
        "    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
        "    --arch fconv --save-dir checpoint/fconv\n",
        "\n",
        "# for sentencepiece remove ▁\n",
        "output = %run 'generate.py' data-bin/enaf --path checpoint/fconv/checkpoint_best.pt --beam 5 --batch-size 128 --remove-bpe --source-lang en --target-lang af"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mt9VPBPqyhIt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**28k**"
      ]
    },
    {
      "metadata": {
        "id": "r9LE_5D8X8Zc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "*   40k: \n",
        "*   36k:\n",
        "*   32k: \n",
        "*   28k:\n",
        "*   24k: \n",
        "*   20k: \n",
        "*   16k: \n",
        "*   12k: \n",
        "*     8k: \n",
        "*     4k: \n",
        "\n"
      ]
    }
  ]
}